{
  "model_info": {
    "model_name": "focal_attention_emotion_classifier",
    "model_type": "audio_emotion_recognition",
    "architecture": "FocalAttentionEmotionClassifier",
    "base_model": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim",
    "training_date": "2025-08-26",
    "version": "1.0"
  },
  "model_performance": {
    "accuracy": 0.375,
    "weighted_f1": 0.373,
    "macro_f1": 0.3204,
    "cohens_kappa": 0.1712,
    "per_class_f1": {
      "anger": 0.4231,
      "joy": 0.1706,
      "melancholy": 0.234,
      "neutral": 0.5144,
      "surprise": 0.2599
    }
  },
  "model_architecture": {
    "hidden_dim": 768,
    "dropout": 0.15,
    "attention_heads": 8,
    "classifier_layers": [
      {
        "type": "LayerNorm",
        "size": 1536
      },
      {
        "type": "Linear",
        "input": 1536,
        "output": 384
      },
      {
        "type": "GELU"
      },
      {
        "type": "Dropout",
        "rate": 0.15
      },
      {
        "type": "Linear",
        "input": 384,
        "output": 128
      },
      {
        "type": "GELU"
      },
      {
        "type": "Dropout",
        "rate": 0.15
      },
      {
        "type": "Linear",
        "input": 128,
        "output": 5
      }
    ]
  },
  "training_config": {
    "focal_loss": {
      "alpha": [
        1.3515,
        0.8599,
        1.2265,
        0.3183,
        1.2438
      ],
      "gamma": 2.0
    },
    "learning_rate": 5e-05,
    "weight_decay": 0.01,
    "batch_size_train": 4,
    "batch_size_eval": 8,
    "num_epochs": 25,
    "gradient_accumulation_steps": 4,
    "warmup_steps": 500,
    "early_stopping_patience": 5,
    "early_stopping_threshold": 0.005
  },
  "data_config": {
    "emotions": [
      "anger",
      "joy",
      "melancholy",
      "neutral",
      "surprise"
    ],
    "label2id": {
      "anger": 0,
      "joy": 1,
      "melancholy": 2,
      "neutral": 3,
      "surprise": 4
    },
    "id2label": {
      "0": "anger",
      "1": "joy",
      "2": "melancholy",
      "3": "neutral",
      "4": "surprise"
    },
    "emotion_mapping_7to5": {
      "neutral": "neutral",
      "surprise": "surprise",
      "fear": "melancholy",
      "sadness": "melancholy",
      "disgust": "melancholy",
      "joy": "joy",
      "anger": "anger"
    },
    "class_distribution": {
      "neutral": 4709,
      "surprise": 1205,
      "melancholy": 1222,
      "joy": 1743,
      "anger": 1109
    }
  },
  "file_paths": {
    "model_directory": "./finetuned-audio-model-5class-emotion-focal-attention/focal_attention_model",
    "config_file": "config.json",
    "weights_file": "pytorch_model.bin",
    "confusion_matrix": "confusion_matrix_focal_attention.png"
  },
  "preprocessing": {
    "input_type": "wav2vec2_hidden_states",
    "input_shape": "[sequence_length, 768]",
    "feature_extraction": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim",
    "required_preprocessing": [
      "Load audio file",
      "Extract wav2vec2 hidden states",
      "Convert to tensor",
      "Apply to(device)"
    ]
  },
  "usage_instructions": {
    "loading_model": [
      "Recreate FocalAttentionEmotionClassifier architecture",
      "Load state_dict from pytorch_model.bin",
      "Set model to eval() mode"
    ],
    "inference_steps": [
      "Load audio hidden states as tensor",
      "Add batch dimension with unsqueeze(0)",
      "Move to appropriate device (cuda/cpu)",
      "Forward pass through model",
      "Apply softmax for probabilities",
      "Get prediction with argmax"
    ]
  },
  "code_templates": {
    "load_model": "\n# Load the trained model\nimport torch\nimport torch.nn as nn\n\nclass FocalAttentionEmotionClassifier(nn.Module):\n    # ... (copy architecture from training script)\n    \nmodel = FocalAttentionEmotionClassifier(None)\nmodel.load_state_dict(torch.load('pytorch_model.bin', map_location='cpu'))\nmodel.eval()\n        ",
    "inference": "\n# Inference on new audio\nhidden_states = torch.load('hidden_state.pt', weights_only=True)\nhidden_states = hidden_states.unsqueeze(0)  # batch dimension\nwith torch.no_grad():\n    _, logits = model(hidden_states)\n    probs = torch.softmax(logits, dim=-1)\n    pred = torch.argmax(logits, dim=-1)\nemotion = ['anger', 'joy', 'melancholy', 'neutral', 'surprise'][pred.item()]\n        "
  },
  "system_requirements": {
    "python": ">=3.8",
    "pytorch": ">=1.12.0",
    "transformers": ">=4.20.0",
    "numpy": ">=1.21.0",
    "recommended_gpu": "CUDA-capable GPU with >=4GB VRAM",
    "minimum_cpu": "Intel i5 or AMD equivalent"
  }
}